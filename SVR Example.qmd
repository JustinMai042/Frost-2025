---
title: "Support Vector Regression with RBF Kernels"
output: html_document
date: "2025-05-9"
---q
---

### Support Vector Regression (SVR)

Support Vector Regression (SVR) is an adaptation of Support Vector Machines which are used for classification to solve regression problems. Instead of finding a line to separate classes, SVR aims to find a function (a line or curve) that best fits the data points, but with a twist.

SVR wants to fit a function to the data such that the predictions are as close as possible to the actual target values, but it allows for a certain amount of error. It tries to find a "tube" around the data points and fits the regression line/curve within this tube, while also trying to keep the tube as "flat" or simple as possible to ensure good generalization to new, unseen data.

The generalized form of the regression function in SVR can be expressed as y=wTx+b, where w is the weight vector, x is the input feature vector, and b is the bias term.

### The Magic of Kernels: The RBF Kernel

Sometimes, the relationship between input features and the output value is non linear. This is where kernels come in. A kernel function allows SVR to model these complex, non linear relationships by mapping the data into a higher dimensional space where a linear like separation or fit becomes easier.

The Radial Basis Function (RBF) kernel, is one of the most popular choices. It's defined as: $$K(x_i, x_j) = \exp(-\gamma \|x_i - x_j\|^2)$$ Where $\|x_i - x_j\|^2$ is the squared Euclidean distance between two data points $x_i$ and $x_j$, and gamma is a parameter that defines how much influence a single training example has. The RBF kernel measures similarity: if two points are close, the kernal value is high (close to 1); if they are far apart, it's low (close to 0). This allows SVR to create decision boundaries or regression lines.

SVR models with RBF kernels are sensitive to the scale of input features. As seen by the distance formula If features are measured on different scales (one feature is from 0 to 1, while another is from 1,000 to 100,000), the feature with the larger numerical range will dominate the distance calculation. This can hide the influence of features with smaller scales, leading to poor model performance. So you want to scale and standardize your input features.

## Core Concepts of SVR with an RBF Kernel

### The $\epsilon$ Insensitive Tube

A key concept in SVR is the $\epsilon$ insensitive tube. Imagine our regression line/curve. SVR defines a margin of error, $\epsilon$, on either side of this curve, forming a "tube."

-   Data points that fall inside this tube (their prediction error is less than $\epsilon$) are not penalized. The model "doesn't care" about errors this small.
-   Data points that fall outside this tube are penalized, and the model tries to minimize these errors.

This $\epsilon$ insensitivity makes SVR robust to small amounts of noise and helps to find a simpler, more general model.

### Hyperparameters:

To get SVR with an RBF kernel to work well, we need to tune some hyperparameters. For svmRad, the main ones are C, $\gamma$, and $\epsilon$.

#### 1. C (Regularization Parameter)

-   The C parameter trades off correct prediction on the training data against the "smoothness" or "simplicity" of the regression function.

-   Small C: Makes the tube wider (more tolerance for errors outside $\epsilon$). This can lead to a smoother, simpler model that might underfit .

-   Large C: Puts a high penalty on errors outside the $\epsilon$ tube, forcing the model to fit the training data more closely. This can lead to a more complex model that might overfit

    (In the paper, svmRad explored C values from $2^{-4}$ to $2^{15}$)

#### 2. $\gamma$ (Kernel Coefficient)

-   $\gamma$ is specific to the RBF kernel and defines how far the influence of a single training example reaches.

-   Small $\gamma$: Means a larger radius of influence. Points further away are considered similar. This leads to a smoother regression function.

-   Large $\gamma$: Means a smaller radius of influence. Only points very close are considered similar. This can lead to a more "wiggly" and complex function that might overfit.

    (The paper noted that for svmRad, while 25 values for a related parameter $\sigma$ (sigma, where $\gamma$ is related to $1/(2\sigma^2)$) were specified, only 6 seemed to be explored by the R tool, which could have impacted its performance)

#### 3. $\epsilon$ (Margin)

-   $\epsilon$ defines the width of the insensitive zone around the predicted function. No penalty is incurred for data points whose prediction error is within this $\epsilon$ margin.
-   Small $\epsilon$: The model will try to fit the data more precisely, as even small deviations are penalized. This can lead to a more complex model.
-   Large $\epsilon$: Allows for greater tolerance, potentially leading to a simpler model but might smooth over important details if too large.

Finding the right values of C, $\gamma$, and $\epsilon$ is usually done through techniques like Grid Search with Cross Validation.

### Support Vectors, The Key Data Points

Not all data points are equally important in defining the SVR model. The data points that lie on the boundary of the $\epsilon$ tube or outside it are called support vectors. These are the critical points that "support" or define the regression function. If you were to move a non support vector point (one inside the tube), the regression function wouldn't change. But moving a support vector would likely change the function. This property can make SVR memory efficient, especially if the number of support vectors is small compared to the total dataset size.

## A Simple SVR RBF Example in R

I am going to try to predict a non linear relationship using the kernLab library.

```{r}

library(kernlab)

library(caTools)

library(tidyverse)


set.seed(123)

# fake wave data
X_vec <- sort(5 * runif(100))
X <- matrix(X_vec, ncol = 1) 

y_true <- sin(X_vec)
y_true <- y_true + 0.5 * (0.5 - runif(length(y_true)))
y <- matrix(y_true, ncol = 1) 

# 80/20 split
set.seed(123) 
split <- sample.split(y, SplitRatio = 0.8)
X_train <- subset(X, split == TRUE)
X_test <- subset(X, split == FALSE)
y_train <- subset(y, split == TRUE)
y_test <- subset(y, split == FALSE)

# center and scale data
X_train_scaled_attrs <- scale(X_train)
X_train_scaled <- X_train_scaled_attrs[,1, drop=FALSE] 
center_X <- attr(X_train_scaled_attrs, "scaled:center")
scale_X <- attr(X_train_scaled_attrs, "scaled:scale")

X_test_scaled <- scale(X_test, center = center_X, scale = scale_X)[,1, drop=FALSE]
X_full_scaled <- scale(X, center = center_X, scale = scale_X)[,1, drop=FALSE]

y_train_scaled_attrs <- scale(y_train)
y_train_scaled <- y_train_scaled_attrs[,1, drop=FALSE] 
center_y <- attr(y_train_scaled_attrs, "scaled:center")
scale_y <- attr(y_train_scaled_attrs, "scaled:scale")
y_train_scaled_ravel <- as.vector(y_train_scaled)

# initialize and train the SVR model 
svr_ksvm_model <- ksvm(x = X_train_scaled, y = y_train_scaled_ravel,
                       type = "eps-svr",      
                       kernel = "rbfdot",     
                       C = 100,               
                       kpar = list(sigma = 0.5), 
                       epsilon = 0.1)


y_pred_scaled <- predict(svr_ksvm_model, X_full_scaled) 
y_pred_scaled_matrix <- matrix(y_pred_scaled[,1], ncol = 1)
y_pred <- (y_pred_scaled_matrix * scale_y) + center_y
y_pred_vec <- as.vector(y_pred)


plot_data_full <- data.frame(X = X_vec, y_true = y_true, y_pred = y_pred_vec)
plot_data_train <- data.frame(X = as.vector(X_train), y = as.vector(y_train), type = "Training data")
plot_data_test <- data.frame(X = as.vector(X_test), y = as.vector(y_test), type = "Test data")



epsilon_val <- svr_ksvm_model@param$epsilon 



epsilon_original_scale <- epsilon_val * scale_y 
plot_data_full$ymin_tube <- plot_data_full$y_pred - epsilon_original_scale
plot_data_full$ymax_tube <- plot_data_full$y_pred + epsilon_original_scale

# support vectors
sv_indices_train <- SVindex(svr_ksvm_model) 

# using the original scaled training data to get SV X values


support_vectors_X_scaled_train <- X_train_scaled[sv_indices_train, , drop = FALSE] 
support_vectors_y_scaled_train <- y_train_scaled[sv_indices_train, , drop = FALSE]

support_vectors_X_original_train <- (support_vectors_X_scaled_train * scale_X) + center_X
support_vectors_y_original_train <- (support_vectors_y_scaled_train * scale_y) + center_y

plot_data_sv <- data.frame(
X_sv = as.vector(support_vectors_X_original_train),
y_sv = as.vector(support_vectors_y_original_train))
    



# plot prediction and points
p_ksvm <- ggplot() +
  geom_point(data = plot_data_train, aes(x = X, y = y, color = type), size = 2, alpha = 0.7) +
  geom_point(data = plot_data_test, aes(x = X, y = y, color = type), size = 2, alpha = 0.7) +
  geom_line(data = plot_data_full, aes(x = X, y = y_pred, linetype = "SVR RBF prediction (ksvm)"), color = "navy", linewidth = 1) 


# SVs
  p_ksvm <- p_ksvm + 
    geom_point(data = plot_data_sv, aes(x = X_sv, y = y_sv, shape = "Support Vectors (Training)"),
               size = 4, fill = NA, color = "black", stroke = 1)

# final combined graph
p_ksvm <- p_ksvm +
  scale_color_manual(name = "Data Type", values = c("Training data" = "orange3", "Test data" = "steelblue")) +
  scale_linetype_manual(name = "Model", values = c("SVR RBF prediction (ksvm)" = "solid")) +
  scale_shape_manual(name = "Model Components", values = c("Support Vectors (Training)" = 1)) + 
  labs(title = "SVR with RBF Kernel",
       x = "Data (X)",
       y = "Target (y)") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5),
        legend.box="vertical") +
  guides(color = guide_legend(title = "Data Type", order = 1),
         linetype = guide_legend(title = "Model", order = 2),
         shape = guide_legend(title = "Model Components", order = 3))

print(p_ksvm)

# test metrics and vectors
num_support_vectors_ksvm <- svr_ksvm_model@nSV
cat(paste("Number of support vectors (ksvm):", num_support_vectors_ksvm, "\n"))


y_test_pred_scaled_ksvm <- predict(svr_ksvm_model, X_test_scaled)
y_test_pred_scaled_ksvm_matrix <- matrix(y_test_pred_scaled_ksvm[,1], ncol = 1)
y_test_pred_ksvm <- (y_test_pred_scaled_ksvm_matrix * scale_y) + center_y
y_test_pred_ksvm_vec <- as.vector(y_test_pred_ksvm)

mse_ksvm <- mean((as.vector(y_test) - y_test_pred_ksvm_vec)^2)
cat(paste("MSE Test Set (ksvm):", sprintf("%.4f", mse_ksvm), "\n"))
```

## The Black Box

SVR models with non linear kernels, like the RBF kernel, are black box models. This is because the RBF kernel transforms the input features into a very high dimensional space. While the regression function is linear in this transformed space, the relationship between the original input features and the final prediction becomes too complex for humans to understand. But if there is no kernel you have, a linear SVR

-   A Linear SVR learns a model of the form:

    Predicted Value = (weight1 \* feature1) + (weight2 \* feature2) + ... + (weightN \* featureN) + bias

-   The weights (weight1, weight2, etc) assigned to each feature have a direct and understandable meaning

-   You can look at these weights and get a good sense of how the model is making its predictions based on the input features. This is very similar to how you interpret coefficients in standard Linear Regression.

## Confidence Intervals for SVR

1.  Generating multiple bootstrap samples by resampling with replacement from the original training dataset.

2.  Fitting an SVR model independently to each bootstrap sample. This creates an ensemble of SVR models.

3.  For a new input instance, predictions are obtained from each model in the ensemble.

4.  The distribution of these predictions is then used to estimate the CI or PI. For example, a 95% PI is constructed from the 2.5 and 97.5th percentiles of the bootstrap predictions.

Interpretation: Based on the svmRad model and the observed data, we are 95% confident that the true value of the predicted outcome will fall within the calculated confidence interval.

## 

| Parameter | Default Value | Explanation |
|----|----|----|
| kernel | **"rbfdot"** (Radial Basis Function kernel) | This is the most common default kernel for ksvm when dealing with numeric data and a specific kernel isn't chosen. |
| C | **1** | N/A |
| kpar | **"automatic"** for rbfdot | Kernel parameters. For rbfdot, if kpar is "automatic" or not specified, ksvm uses estimates a suitable sigma value. |
| epsilon | **0.1** | Defines the width of the tube around the regression function where errors are not penalized. |
| scaled | **TRUE** | TRUE, input data (x and y variables if applicable) are internally scaled and stored. |

| Pros | Cons |
|----|----|
| Handles Many Problems: Can be used for sorting data into groups (classification), predicting numbers (regression), and finding unusual data. | Tricky Settings: Figuring out the best hyperparams (C, epsilon, and kernel choices) can be hard and take time. You might need to try many options. |
| Works with Complex Data: Good at finding patterns even if the relationships in your data are not simple straight lines, thanks to different kernels. | Can Be Slow: If you have a very large amount of data, training the model can take a long time. |
| Lots of Kernel Choices: You can pick from many types of "kernels" to best fit your data, or even create your own. | "Black Box" Model: It can be hard to understand exactly why the model makes a particular prediction especially with complex kernels. |
| Often Accurate: When set up well, SVMs can make very accurate predictions. | Needs Data Scaled: Usually works best if you scale your input numbers first. ksvm can do this automatically, but it's something to be aware of. |
| Built in Extras: Has some helpful built in features like automatic data scaling and options for CV to test your model. | Memory Use: Can use a lot of computer memory, especially with large datasets |

Add outliers, SV

Sources unformattted:

-   <https://pubmed.ncbi.nlm.nih.gov/30654138/>

-   <https://www.ncbi.nlm.nih.gov/books/NBK583961/>

-   <http://www.columbia.edu/~mh2078/MachineLearningORFE/SVMs_MasterSlides.pdf>

-   <https://www.geeksforgeeks.org/rbf-svm-parameters-in-scikit-learn/>

-   <https://towardsdatascience.com/radial-basis-function-rbf-kernel-the-go-to-kernel-acf0d22c798a/>

-   <https://www.reddit.com/r/MachineLearning/comments/3zqwbc/where_do_support_vector_machines_perform_badly/>

```{r}
times <- c(79, 75, 82, 83, 69, 65, 49, 62, 77, 67,
           66, 59, 80, 57, 86, 55, 80, 74, 77, 63)

n_pos <- sum(times > 60)
n_neg <- sum(times < 60)
n <- n_pos + n_neg

# two-sided binomial test
binom.test(min(n_pos, n_neg), n = n, p = 0.5, alternative = "two.sided")

```

```{r}
2
wilcox.test(high, low, alternative = "greater", paired = TRUE)


```

```{r}
wilcox.test(times, mu = 60, alternative = "two.sided")

```

```{r}
t.test(times, mu = 60, alternative = "two.sided")
1

```
