---
title: "Example Workflow"
author: "Colin Hassett"
date: "2025-07-29"
output: html_document
---

## Example Workflow

# Data Splitting

```{r}
stab_df <- read.table("C:/Users/nerdc/Downloads/file22f1652de1c8a.txt", sep = ",", skip = 16, col.names = c("tau1", "tau2", "tau3", "tau4", "p1", "p2", "p3", "p4", "g1", "g2", "g3", "g4", "stab", "stabf"))
stab_df
```
```{r}
compress_df <- read.table("C:/Users/nerdc/OneDrive/Desktop/regression/data/compress_stren/compress_stren_R.dat", header = TRUE)

compress_df
```


```{r}
set.seed(123)
split_obj <- initial_validation_split(
  compress_df,
  strata = NULL,
  prop = c(0.6, 0.2),
  pool = 0.1
)

train_full <- training(split_obj)
valid_data <- validation(split_obj)
test_data  <- testing(split_obj)
```

```{r}
rec <- recipe(output ~ ., data = train_full) |>
  step_rm(all_predictors(), -all_numeric()) |>
  step_dummy(all_nominal(), one_hot = TRUE) |>
  step_zv(all_predictors()) |>
  step_corr(all_numeric(), threshold = 0.9) |>
  step_normalize(all_numeric())

```

# Model Specifications

```{r}
library(rules)

knn_spec <- nearest_neighbor(
  neighbors = tune(),      # number of neighbors (k)
  weight_func = "rectangular",    # how neighbors are weighted: "rectangular", "triangular", etc.
  dist_power = 2      # Minkowski distance power: 1 = Manhattan, 2 = Euclidean
) %>%
  set_engine("kknn") %>%
  set_mode("regression")

svmrad_spec <- svm_rbf(
  cost = tune(),
  rbf_sigma = tune()
) %>%
  set_engine("kernlab") %>%
  set_mode("regression")

cubist_spec <- cubist_rules(
  committees = tune(),  # number of boosting iterations
  neighbors = tune()    # local rules to average
) %>%
  set_engine("Cubist") %>%
  set_mode("regression")

linear_spec <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

boost_spec <- boost_tree(
  trees = tune(),
  tree_depth = tune(),
  learn_rate = 0.1
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

lasso_spec <- linear_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet") %>%
  set_mode("regression")

ridge_spec <- linear_reg(penalty = tune(), mixture = 0) %>%
  set_engine("glmnet") %>%
  set_mode("regression")

enet_spec <- linear_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet") %>%
  set_mode("regression")

rforest_spec <- rand_forest(
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

mlp_spec <- mlp(
  hidden_units = tune(),  # like `size`
  penalty = tune(),       # like `decay`
  epochs = 100            # fixed number of iterations (default for `nnet`)
) %>%
  set_engine("nnet") %>%
  set_mode("regression")

mars_spec <- bag_mars(num_terms = tune()) %>%
  set_engine("earth") %>%
  set_mode("regression")

bart_spec <- bart(trees = tune()) %>%
  set_mode("regression") %>%
  set_engine("dbarts")

model_specs <- list(
  boost = boost_spec,
  lasso = lasso_spec,
  ridge = ridge_spec,
  enet = enet_spec,
  rforest = rforest_spec,
  mlp = mlp_spec,
  mars = mars_spec,
  bart = bart_spec,
  linear = linear_spec,
  cubist = cubist_spec,
  svmrad = svmrad_spec,
  knn = knn_spec
)
```


```{r}
library(baguette)
library(tidyr)
library(kernlab)
library(kknn)

all_results <- list()

for (dataset_name in names(openml_datasets_list)) {
  df <- openml_datasets_list[[dataset_name]]
  
  split_obj <- initial_validation_split(
    df,
    strata = NULL,
    prop = c(0.6, 0.2),
    pool = 0.1
  )

  train_full <- training(split_obj)
  #valid_data <- validation(split_obj)
  test_data  <- testing(split_obj)
  val_rs <- validation_set(split_obj)
  
  # Prep the recipe once for reuse
  prep_rec <- prep(rec, training = train_full)
  train_data <- bake(prep_rec, new_data = NULL)
  
  dataset_results <- list()
  
  for (model_name in names(model_specs)) {
    cat("Fitting", model_name, "on", dataset_name, "\n")
    
    current_spec <- model_specs[[model_name]]
    
    wf <- workflow() |>
      add_recipe(rec) |>
      add_model(current_spec)
   
    # Extract and finalize parameters
    params <- extract_parameter_set_dials(wf)
    
    if (nrow(params) > 0) {
      if ("mtry" %in% params$id) {
      mtry_data <- bake(prep_rec, new_data = NULL) |>
        select(-target)  # Remove outcome
      params <- finalize(params, mtry_data)
      }
  
      grid <- grid_latin_hypercube(
        params,
        size = 20
        )
      start_time <- Sys.time()
      
      tune_res <- tune_grid(
        wf,
        resamples = val_rs,
        grid = grid,
        metrics = metric_set(rmse, rsq, mae)
      )
      
      best_params <- select_best(tune_res, metric = "rmse")
    
      final_wf <- finalize_workflow(wf, best_params)
      
      final_fit <- last_fit(
        final_wf,
        split = split_obj
      )
      
      end_time <- Sys.time()
    } else {
      start_time <- Sys.time()
      
      final_fit <- last_fit(wf, split = split_obj)
      
      end_time <- Sys.time()
    }
    
    duration <- as.numeric(difftime(end_time, start_time, units = "secs"))
    
    metrics <- collect_metrics(final_fit) |>
      select(.metric, .estimate) |>
      pivot_wider(names_from = .metric, values_from = .estimate) |>
      mutate(model = model_name,
             data_set = dataset_name,
             time_secs = duration
             )
    
    dataset_results[[model_name]] <- metrics
  }
  
  dataset_df <- bind_rows(dataset_results) %>%
    relocate(dataset, model, .before = everything()) %>%
    mutate(
      rank_rsq = min_rank(desc(rsq)),
      rank_time = min_rank(time_secs)
    )

  all_results[[dataset_name]] <- dataset_df
    
}

final_results <- bind_rows(all_results)

final_results

```



