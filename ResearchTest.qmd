---
title: "SimData with methods"
format: html
editor: visual
---

```{r}
library(simstudy)



def <- defData(varname = "x1", formula = "0;1", dist = "uniform")
def <- defData(def,    varname = "x2", formula = "0;1", dist = "uniform")
def <- defData(def,    varname = "x3", formula = 0,     variance = 1, dist = "normal")
def <- defData(def,    varname = "x4", formula = 0,     variance = 1, dist = "t", df = 3)
def <- defData(def,    varname = "x5", formula = "0.8*x1", variance = 0.2, dist = "normal")
def <- defData(def,    varname = "x6", formula = "0.8*x2", variance = 0.2, dist = "normal")

def <- defData(def, varname = "mu",
               formula = "sin(2*x1*x2) + (x3 - 0.5)^2 + 0.5 + 2*x5*x6",
               dist    = "nonrandom")
def <- defData(def, varname = "sigma",
               formula = "1 + 2*abs(x3)",
               dist    = "nonrandom")

# 3) Heavy-tailed response: t-distributed around μ with scale σ and df=3
def <- defData(def, varname   = "y",
               formula   = "mu",
               variance  = "sigma",
               dist      = "t",
               df        = 3)

# 4) Generate data
sim_df <- genData(1000, def)

```

```{r}
library(farff)

grid_stability <- readARFF("/Users/justinmai/Downloads/file22f1652de1c8a.arff", data.reader = "readr", tmp.file = tempfile())
```

```{r}
library(tidymodels)

# 1. Initial three-way split (train / validation / test)
set.seed(123)
split_obj <- initial_validation_split(
  df,
  strata = NULL,
  prop = c(0.6, 0.2) ,      # 60% train, 20% validation, 20% test
  pool = 0.1
)                           

train_full <- training(split_obj)
valid_data <- validation(split_obj)
test_data  <- testing(split_obj)

# 2. Preprocessing recipe
rec <- recipe(y ~ ., data = train_full) |>
  step_rm(all_predictors(), -all_numeric()) |>
  step_dummy(all_nominal(), one_hot = TRUE) |>
  step_zv(all_predictors()) |>
  step_corr(all_numeric(), threshold = 0.9) |>
  step_normalize(all_numeric())

# 3. Model specification (XGBoost regression)
xgb_spec <- boost_tree(
  trees      = tune(),
  tree_depth = tune(),
  learn_rate = 0.1
) |>
  set_engine("xgboost") |>
  set_mode("regression")

# 4. Workflow assembly
wf <- workflow() |>
  add_recipe(rec) |>
  add_model(xgb_spec)

# 5. Hyperparameter grid 
grid <- grid_latin_hypercube(
  extract_parameter_set_dials(wf),
  size = 20
)

# 6. Tune on the train/validation split
#    We wrap train_full & valid_data into a single validation set
val_rs <- validation_set(split_obj)  
tune_res <- tune_grid(
  wf,
  resamples = val_rs,
  grid      = grid,
  metrics   = metric_set(rmse, rsq, mae)
)

# 7. Select best parameters
best_params <- select_best(tune_res, "rmse")

# 8. Finalize workflow with chosen hyperparameters
final_wf <- finalize_workflow(wf, best_params)

# 9. Fit on train+validation, test on held-out data
final_fit <- last_fit(
  final_wf,
  initial_split(df, prop = 0.8)      # 80% train+valid, 20% test
)

# 10. Collect and display metrics
collect_metrics(final_fit)
```

```{r}

grid_fit <- lm(stab ~., data=grid_stability)

plot(grid_fit)
```

```{r}
library(dplyr)
library(tidyr)

wave_energy<-readARFF("/Users/justinmai/Downloads/file22f16310c401a.arff", data.reader = "readr", tmp.file = tempfile())

wave_fit <- lm(energy_total~., data = wave_energy)

plot(wave_fit)


cor_matrix <- cor(wave_energy)

highly_correlated_pair <- cor_matrix %>%
  as.data.frame() %>%
  tibble::rownames_to_column(var = "var1") %>%
  pivot_longer(-var1, names_to = "var2", values_to = "correlation") %>%
  filter(abs(correlation) > 0.75 & var1 != var2) %>%
  mutate(sorted_vars = purrr::map2_chr(var1, var2, ~ paste(sort(c(.x, .y)), collapse = "_"))) %>%
  distinct(sorted_vars, .keep_all = TRUE) %>%
  select(var1, var2, correlation)

# Print the resulting pairs
print(highly_correlated_pair)

```

```{r}

video_trans<-readARFF("/Users/justinmai/Downloads/file22f1662968585.arff", data.reader = "readr", tmp.file = tempfile())

video_fit <- lm(utime ~., data=video_trans)
plot(video_fit)

```

```{r}

library(MASS)

numeric_data <- video_trans[, sapply(video_trans, is.numeric)]

numeric_data <- na.omit(numeric_data)

inv_cov <- ginv(cov(numeric_data))

md <- mahalanobis(
  x = numeric_data,
  center = colMeans(numeric_data),
  cov = inv_cov,
  inverted = TRUE
)

threshold <- qchisq(p = 0.975, df = ncol(numeric_data) - 1)

# Identify outliers
outliers <- which(md > threshold)

# How many outliers?
length(outliers)
```

```{r}
video_trans_numeric <- select(video_trans, where(is.numeric))


cor_matrix <- cor(video_trans_numeric)

highly_correlated_pair <- cor_matrix %>%
  as.data.frame() %>%
  tibble::rownames_to_column(var = "var1") %>%
  pivot_longer(-var1, names_to = "var2", values_to = "correlation") %>%
  filter(abs(correlation) > 0.75 & var1 != var2) %>%
  mutate(sorted_vars = purrr::map2_chr(var1, var2, ~ paste(sort(c(.x, .y)), collapse = "_"))) %>%
  distinct(sorted_vars, .keep_all = TRUE) %>%
  select(var1, var2, correlation)


print(highly_correlated_pair)


```

```{r}

sarcos<-readARFF("/Users/justinmai/Downloads/file22f166a1669bb.arff", data.reader = "readr", tmp.file = tempfile())

sarcos_fit <- lm(V22~., data = sarcos)
plot(sarcos_fit)


#/////////////////////////////////////////////////////////


numeric_data <- sarcos[, sapply(sarcos, is.numeric)]

numeric_data <- na.omit(numeric_data)

inv_cov <- ginv(cov(numeric_data))

md <- mahalanobis(
  x = numeric_data,
  center = colMeans(numeric_data),
  cov = inv_cov,
  inverted = TRUE
)

threshold <- qchisq(p = 0.975, df = ncol(numeric_data) - 1)


outliers <- which(md > threshold)


length(outliers)


#/////////////////////////////////////////////////////////

sarcos_numeric <- select(sarcos, where(is.numeric))


cor_matrix <- cor(sarcos_numeric)

highly_correlated_pair <- cor_matrix %>%
  as.data.frame() %>%
  tibble::rownames_to_column(var = "var1") %>%
  pivot_longer(-var1, names_to = "var2", values_to = "correlation") %>%
  filter(abs(correlation) > 0.75 & var1 != var2) %>%
  mutate(sorted_vars = purrr::map2_chr(var1, var2, ~ paste(sort(c(.x, .y)), collapse = "_"))) %>%
  distinct(sorted_vars, .keep_all = TRUE) %>%
  select(var1, var2, correlation)


print(highly_correlated_pair)


```

```{r}



sarcos<-readARFF("/Users/justinmai/Downloads/file22f1653ed424d.arff", data.reader = "readr", tmp.file = tempfile())
sarcos_fit <- lm(median_housing~., data = sarcos)
plot(sarcos_fit)



#/////////////////////////////////////////////////////////


numeric_data <- sarcos[, sapply(sarcos, is.numeric)]

numeric_data <- na.omit(numeric_data)

inv_cov <- ginv(cov(numeric_data))

md <- mahalanobis(
  x = numeric_data,
  center = colMeans(numeric_data),
  cov = inv_cov,
  inverted = TRUE
)

threshold <- qchisq(p = 0.975, df = ncol(numeric_data) - 1)


outliers <- which(md > threshold)


length(outliers)


#/////////////////////////////////////////////////////////

sarcos_numeric <- select(sarcos, where(is.numeric))


cor_matrix <- cor(sarcos_numeric)

highly_correlated_pair <- cor_matrix %>%
  as.data.frame() %>%
  tibble::rownames_to_column(var = "var1") %>%
  pivot_longer(-var1, names_to = "var2", values_to = "correlation") %>%
  filter(abs(correlation) > 0.75 & var1 != var2) %>%
  mutate(sorted_vars = purrr::map2_chr(var1, var2, ~ paste(sort(c(.x, .y)), collapse = "_"))) %>%
  distinct(sorted_vars, .keep_all = TRUE) %>%
  select(var1, var2, correlation)


print(highly_correlated_pair)

```

```{r}

library(MASS)   
library(simstudy)

set.seed(42)

N  <- 10000
p  <- 12
rho <- 0.7 


Sigma <- diag(p)
pairs <- list(c(1,2), c(3,4), c(5,6))
for(pr in pairs) {
  Sigma[pr[1], pr[2]] <- rho
  Sigma[pr[2], pr[1]] <- rho
}


X <- mvrnorm(N, mu = rep(0, p), Sigma = Sigma)
colnames(X) <- paste0("X", 1:p)
df <- as.data.frame(X)


error_sd      <- sqrt(1 + 2 * df$X1^2)
df$epsilon    <- rnorm(N, 0, error_sd)


df$Y <- 1 + 1.5*df$X1 - 2*df$X2 + 0.5*df$X3 + df$epsilon


out_idx <- sample.int(N, 500)
df$Y[out_idx] <- df$Y[out_idx] + rnorm(500, mean = 0, sd = 10)


summary(df$Y)
```

```{r}

library(tidymodels)
library(rules)
library(Cubist)
library(rlang)


run_ml_analysis <- function(model_spec,
                            data,
                            target = "Y",
                            train_prop = 0.60,
                            val_prop = 0.20,
                            grid_size = 50,
                            seed = 123) {

  if (!target %in% names(data))
    stop(paste("Column `{target}` not found."))

  formula_obj <- as.formula(paste(target, "~ ."))
  target_sym  <- sym(target)

  set.seed(seed)
  split1     <- initial_split(data, prop = train_prop + val_prop, strata = !!target_sym)
  train_val  <- training(split1)
  test_set   <- testing(split1)

  folds <- vfold_cv(train_val, v = 2, strata = !!target_sym)

  rec <- recipe(formula_obj, data = train_val) |>
    step_dummy(all_nominal_predictors(), one_hot = TRUE) |>
    step_zv(all_predictors()) |>
    step_corr(all_numeric_predictors(), threshold = 0.6) |>
    step_normalize(all_numeric_predictors())

  wf <- workflow() |>
    add_recipe(rec) |>
    add_model(model_spec)

  grid <- grid_latin_hypercube(parameters(wf), size = grid_size)

  tune_res <- tune_grid(
    wf,
    resamples = folds,
    grid      = grid,
    metrics   = metric_set(rmse, rsq, mae)
  )

  best <- select_best(tune_res, metric = "rmse")
  final_wf  <- finalize_workflow(wf, best)
  final_fit <- last_fit(final_wf, split1)

  list(
    metrics   = collect_metrics(final_fit),
    best_pars = best,
    workflow  = extract_workflow(final_fit)
  )
}

cub_spec <- cubist_rules(
  committees = tune(),
  neighbors  = tune()
) |>
  set_engine("Cubist") |>
  set_mode("regression")

result_cub <- run_ml_analysis(
  model_spec = cub_spec,
  data       = df,
  target     = "Y"
)

result_cub$metrics

```

```{r}
lmod <- lm(Y~. ,df)

plot(lmod)
```
